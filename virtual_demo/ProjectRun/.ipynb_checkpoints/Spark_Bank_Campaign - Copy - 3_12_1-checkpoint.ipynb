{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3ac9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b18317",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\Lemonrice\\virtual_demo\\ProjectRun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74dedd",
   "metadata": {},
   "source": [
    "# Using API from Kaggle to download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce9a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading bank-telemarketing-dataset.zip to D:\\Lemonrice\\virtual_demo\\ProjectRun\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/905k [00:00<?, ?B/s]\n",
      "100%|##########| 905k/905k [00:02<00:00, 443kB/s]\n",
      "100%|##########| 905k/905k [00:02<00:00, 443kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d princeganer/bank-telemarketing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991b4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  bank-telemarketing-dataset.zip\n",
      "  inflating: bank-additional-full.csv  \n",
      "  inflating: bank-additional-names.txt  \n",
      "  inflating: bank-full.csv           \n",
      "  inflating: info.txt                \n"
     ]
    }
   ],
   "source": [
    "!unzip bank-telemarketing-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25146802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Storage\n",
      " Volume Serial Number is E457-CEE6\n",
      "\n",
      " Directory of D:\\Lemonrice\\virtual_demo\\ProjectRun\n",
      "\n",
      "12/03/2023  11:18    <DIR>          .\n",
      "12/03/2023  11:18    <DIR>          ..\n",
      "12/03/2023  11:17    <DIR>          .ipynb_checkpoints\n",
      "10/03/2023  08:52         5,834,924 bank-additional-full.csv\n",
      "10/03/2023  08:52             5,458 bank-additional-names.txt\n",
      "10/03/2023  08:52         4,610,348 bank-full.csv\n",
      "12/03/2023  11:18           927,049 bank-telemarketing-dataset.zip\n",
      "10/03/2023  08:52                94 info.txt\n",
      "12/03/2023  11:09            53,604 Spark_Bank_Campaign - Copy - 3_12_1.ipynb\n",
      "               6 File(s)     11,431,477 bytes\n",
      "               3 Dir(s)  75,674,648,576 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868dc64",
   "metadata": {},
   "source": [
    "Sucessfully imported data from Kaggle API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fc781",
   "metadata": {},
   "source": [
    "## Importing libraries for further operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705bd23d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m when, count\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import when, count\n",
    "from pyspark.sql.functions import col, sum\n",
    "import pyspark.pandas as ps\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8523fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sparksession\n",
    "spark = SparkSession.builder.master('local[4]').appName('ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5390a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display function\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display maximum columns in pandas\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e794eb1",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fb2b3",
   "metadata": {},
   "source": [
    "## Bank Full (dataset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark Dataframe\n",
    "bank_full = spark.read.csv('bank-full.csv', sep = \";\" , header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588df899",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b019844",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c29062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the index columns \n",
    "\n",
    "new_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor_3m\", \"nr_employed\"]\n",
    "for column in new_cols:\n",
    "    bank_full = bank_full.withColumn(column, bank_full[\"poutcome\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map years into a dataframe\n",
    "\n",
    "def year_mapper(data, start_yr, end_yr):\n",
    "    \"\"\"\n",
    "        This function takes dataframe, start year of data, end year of data as input and\n",
    "        returns a new dataframe having year column mapped to it.\n",
    "    \"\"\"\n",
    "    month_lst = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "    # Make a copy of the original dataframe\n",
    "    new_data = data.copy()\n",
    "\n",
    "    # Insert a new \"year\" column filled with zeros\n",
    "    new_data.insert(loc=0, column=\"year\", value=0)\n",
    "\n",
    "    # Set the first year to the start year\n",
    "    current_year = int(start_yr)\n",
    "    new_data.at[0, \"year\"] = current_year\n",
    "\n",
    "    # Loop through the rows of the dataframe, updating the year column when the month changes\n",
    "    for i in range(1, len(new_data)):\n",
    "        # If the current month is earlier in the year than the previous month, increment the year\n",
    "        if month_lst.index(new_data[\"month\"][i]) < month_lst.index(new_data[\"month\"][i-1]):\n",
    "            current_year += 1\n",
    "\n",
    "        new_data.at[i, \"year\"] = current_year\n",
    "\n",
    "        # If the current year exceeds the end year, break out of the loop\n",
    "        if current_year > end_yr:\n",
    "            break\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Use default index prevent overhead.\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "\n",
    "# Ignore warnings coming from Arrow optimizations.\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "# To speed up dataset processing\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "bank_full_pdf = bank_full.toPandas()\n",
    "\n",
    "# Apply the function to the Pandas DataFrame\n",
    "new_pandas_df = year_mapper(bank_full_pdf, 2008, 2010)\n",
    "\n",
    "# Convert the updated Pandas DataFrame back to a PySpark DataFrame\n",
    "bank_full = spark.createDataFrame(new_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full.groupBy(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d7184",
   "metadata": {},
   "source": [
    "### Index mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the missing index into the dataframes because it have the same values along the months\n",
    "\n",
    "def map_index(new_data):\n",
    "\n",
    "    index_2008 = {\"may\":{\"emp_var_rate\":1.1, \"cons_price_idx\":93.994, \"cons_conf_idx\":-36.4, \"euribor_3m\":4.85, \"nr_employed\":5191},\n",
    "                 \"jun\":{\"emp_var_rate\":1.4, \"cons_price_idx\":94.465, \"cons_conf_idx\":-41.8, \"euribor_3m\":4.86, \"nr_employed\":5228.1},\n",
    "                 \"jul\":{\"emp_var_rate\":1.4, \"cons_price_idx\":93.918, \"cons_conf_idx\":-42.7, \"euribor_3m\":4.96, \"nr_employed\":5228.1},\n",
    "                 \"aug\":{\"emp_var_rate\":1.4, \"cons_price_idx\":93.444, \"cons_conf_idx\":-36.1, \"euribor_3m\":4.965, \"nr_employed\":5228.1},\n",
    "                 \"oct\":{\"emp_var_rate\":-0.1, \"cons_price_idx\":93.798, \"cons_conf_idx\":-40.4, \"euribor_3m\":5, \"nr_employed\":5195.8},\n",
    "                 \"nov\":{\"emp_var_rate\":-0.1, \"cons_price_idx\":93.2, \"cons_conf_idx\":-42, \"euribor_3m\":4.406, \"nr_employed\":5195.8},\n",
    "                 \"dec\":{\"emp_var_rate\":-0.2, \"cons_price_idx\":92.75, \"cons_conf_idx\":-45.9, \"euribor_3m\":3.563, \"nr_employed\":5176.3}}\n",
    "\n",
    "    index_2009 = {\"jan\":{\"emp_var_rate\":-0.2, \"nr_employed\":5176.3},\n",
    "                  \"feb\":{\"emp_var_rate\":-0.2, \"nr_employed\":5176.3},\n",
    "                  \"mar\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":92.84, \"cons_conf_idx\":-50, \"euribor_3m\":1.811, \"nr_employed\":5099.1},\n",
    "                  \"apr\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":93.075, \"cons_conf_idx\":-47.1, \"euribor_3m\":1.498, \"nr_employed\":5099.1},\n",
    "                  \"may\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":92.89, \"cons_conf_idx\":-46.2, \"euribor_3m\":1.334, \"nr_employed\":5099.1},\n",
    "                 \"jun\":{\"emp_var_rate\":-2.9, \"cons_price_idx\":92.963, \"cons_conf_idx\":-40.8, \"euribor_3m\":1.26, \"nr_employed\":5076.2},\n",
    "                 \"jul\":{\"emp_var_rate\":-2.9, \"cons_price_idx\":93.469, \"cons_conf_idx\":-33.6, \"euribor_3m\":1.072, \"nr_employed\":5076.2},\n",
    "                 \"aug\":{\"emp_var_rate\":-2.9, \"cons_price_idx\":92.201, \"cons_conf_idx\":-31.4, \"euribor_3m\":0.884, \"nr_employed\":5076.2},\n",
    "                 \"sep\":{\"emp_var_rate\":-3.4, \"cons_price_idx\":92.379, \"cons_conf_idx\":-29.8, \"euribor_3m\":0.813, \"nr_employed\":5017.5},\n",
    "                 \"oct\":{\"emp_var_rate\":-3.4, \"cons_price_idx\":92.431, \"cons_conf_idx\":-26.9, \"euribor_3m\":0.754, \"nr_employed\":5017.5},\n",
    "                 \"nov\":{\"emp_var_rate\":-3.4, \"cons_price_idx\":92.649, \"cons_conf_idx\":-30.1, \"euribor_3m\":0.722, \"nr_employed\":5017.5},\n",
    "                 \"dec\":{\"emp_var_rate\":-3.0, \"cons_price_idx\":92.713, \"cons_conf_idx\":-33, \"euribor_3m\":0.718, \"nr_employed\":5023.5}}\n",
    "\n",
    "    index_2010 = {\"jan\":{\"emp_var_rate\":-3.0, \"nr_employed\":5023.5},\n",
    "                  \"feb\":{\"emp_var_rate\":-3.0, \"nr_employed\":5023.5},\n",
    "                   \"mar\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":92.369, \"cons_conf_idx\":-34.8, \"euribor_3m\":0.655, \"nr_employed\":5008.7},\n",
    "                  \"apr\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":93.749, \"cons_conf_idx\":-34.6, \"euribor_3m\":0.64, \"nr_employed\":5008.7},\n",
    "                  \"may\":{\"emp_var_rate\":-1.8, \"cons_price_idx\":93.876, \"cons_conf_idx\":-40, \"euribor_3m\":0.668, \"nr_employed\":5008.7},\n",
    "                 \"jun\":{\"emp_var_rate\":-1.7, \"cons_price_idx\":94.055, \"cons_conf_idx\":-39.8, \"euribor_3m\":0.704, \"nr_employed\":4991.6},\n",
    "                 \"jul\":{\"emp_var_rate\":-1.7, \"cons_price_idx\":94.215, \"cons_conf_idx\":-40.3, \"euribor_3m\":0.79, \"nr_employed\":4991.6},\n",
    "                 \"aug\":{\"emp_var_rate\":-1.7, \"cons_price_idx\":94.027, \"cons_conf_idx\":-38.3, \"euribor_3m\":0.898, \"nr_employed\":4991.6},\n",
    "                 \"sep\":{\"emp_var_rate\":-1.1, \"cons_price_idx\":94.199, \"cons_conf_idx\":-37.5, \"euribor_3m\":0.886, \"nr_employed\":4963.6},\n",
    "                 \"oct\":{\"emp_var_rate\":-1.1, \"cons_price_idx\":94.601, \"cons_conf_idx\":-49.5, \"euribor_3m\":0.959, \"nr_employed\":4963.6},\n",
    "                 \"nov\":{\"emp_var_rate\":-1.1, \"cons_price_idx\":94.767, \"cons_conf_idx\":-50.8, \"euribor_3m\":1.05, \"nr_employed\":4963.6}}\n",
    "\n",
    "    indx = [index_2008, index_2009, index_2010]\n",
    "    years = [2008, 2009, 2010]\n",
    "    \n",
    "    for i in range(len(years)):\n",
    "        for months, indexes in indx[i].items():\n",
    "            for index, index_val in indexes.items():\n",
    "                new_data = new_data.withColumn(index, \n",
    "                    when((col('year') == years[i]) & (col('month') == months), index_val).otherwise(col(index))) \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the index_mapper function\n",
    "bank_full = map_index(new_data = bank_full)\n",
    "display(bank_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab121bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the balance, day column\n",
    "bank_full = bank_full.drop(\"balance\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the dataframes into the pandas\n",
    "dataframe_1 = bank_full.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd769970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bddbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe_1.to_csv('dataframe_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31352b58",
   "metadata": {},
   "source": [
    "### Bank Full Dataset (dataset-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a348f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark Dataframe\n",
    "bank_add_full = spark.read.csv('bank-additional-full.csv', sep=\";\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_add_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_add_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ab48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default index prevent overhead.\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations.\n",
    "\n",
    "# To speed up dataset processing\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "bank_full_pdf = bank_add_full.toPandas()\n",
    "\n",
    "# Apply the function to the Pandas DataFrame\n",
    "new_pandas_df = year_mapper(bank_full_pdf, 2008, 2010)\n",
    "\n",
    "# Convert the updated Pandas DataFrame back to a PySpark DataFrame\n",
    "bank_add_full = spark.createDataFrame(new_pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe0c9f",
   "metadata": {},
   "source": [
    "### replace values from 999 to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes the values from 999 to -1\n",
    "bank_add_full = bank_add_full.withColumn(\"pdays\", when(col(\"pdays\") == 999, -1).otherwise(col(\"pdays\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cad9ea",
   "metadata": {},
   "source": [
    "### Renaming columns names and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a230d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the columns names\n",
    "old_col_list = [\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"]\n",
    "for i in range(0, len(old_col_list)):\n",
    "    bank_add_full = bank_add_full.withColumnRenamed(old_col_list[i], new_cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the categories names from education columns\n",
    "old_edu = [\"basic.4y\", \"high.school\", \"basic.6y\", \"basic.9y\", \"university.degree\", \"professional.course\"]\n",
    "new_edu = [\"basic_4y\", \"high_school\", \"basic_6y\", \"basic_9y\", \"university_degree\" ,\"professional_course\"]\n",
    "\n",
    "for i in range(0,6):\n",
    "    bank_add_full = bank_add_full.withColumn(\"education\", when(col(\"education\") == old_edu[i], new_edu[i]).otherwise(col(\"education\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_add_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22410925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the column day_of week\n",
    "bank_add_full = bank_add_full.drop(\"day_of_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2 = bank_add_full.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe_2.to_csv('dataframe_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cefcd",
   "metadata": {},
   "source": [
    "## Concat two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating two dataframes\n",
    "frames  = [dataframe_1, dataframe_2]\n",
    "\n",
    "bank = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bank.to_csv('final_data.csv')\n",
    "bank.to_parquet(\"bank_data_pdf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet file\n",
    "bank_data = spark.read.parquet('bank_data_pdf.parquet')\n",
    "#bank_data = spark.read.csv('final_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping id\n",
    "bank_data = bank_data.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_data.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e67210",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the summary of dataframes with std, means and quartiles\n",
    "bank_data.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the continuous and categorical variables\n",
    "cat_col = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"year\",\"y\"]\n",
    "cont_col = [\"age\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"emp_var_rate\",\"cons_price_idx\",\"cons_conf_idx\",\"euribor_3m\",\"nr_employed\"]\n",
    "categories = bank_data.select(cat_col)\n",
    "continuous = bank_data.select(cont_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a8de0",
   "metadata": {},
   "source": [
    "### Unique value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the value counts for categrical columns\n",
    "for columns in categories:\n",
    "    print(\"Column Name\", columns)\n",
    "    print(\"-----------------------\")\n",
    "    counts = bank_data.groupBy(columns).count()\n",
    "    counts.show()\n",
    "    print(\"     \")\n",
    "    print(\"******************************************************\")\n",
    "    print(\"     \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfac811",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da98fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename .admin category to admin\n",
    "bank_data = bank_data.withColumn(\"job\", when(col(\"job\") == \"admin.\", \"admin\").otherwise(col(\"job\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing \"unknown\" and \"nonexistent\" with the null values\n",
    "for column in bank_data.columns:\n",
    "    bank_data = bank_data.withColumn(column, when(col(column).isin(\"unknown\", \"nonexistent\"), None).otherwise(col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_data.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2594f1",
   "metadata": {},
   "source": [
    "### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the null values for categorical values\n",
    "bank_data.agg(*[count(when(col(c).isNull(), c)).alias(c) for c in categories.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the null values for continuous values\n",
    "bank_data.agg(*[count(when(col(c).isNull(), c)).alias(c) for c in continuous.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c373676",
   "metadata": {},
   "source": [
    "### Replacing continue variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bfbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# calculate the mean of non-null values in columns\n",
    "mean_dict = bank_data.select(*(mean(c).alias(c) for c in cont_col)).first().asDict()\n",
    "\n",
    "# replace null values with the mean\n",
    "bank_data = bank_data.fillna(mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "bank_data.agg(*[count(when(col(c).isNull(), c)).alias(c) for c in continuous.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6f733",
   "metadata": {},
   "source": [
    "### Replacing categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_data.drop(\"poutcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772755e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mode of non-null values and replaced in columns\n",
    "from pyspark.sql.functions import desc\n",
    "for column in cat_col:\n",
    "    mode = bank_data.groupBy(column).agg(count(\"*\").alias(\"count\")).orderBy(desc(\"count\")).select(column).first()[0]\n",
    "    bank_data = bank_data.fillna({column: mode})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7192b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "bank_data.agg(*[count(when(col(c).isNull(), c)).alias(c) for c in categories.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=bank_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3423c0",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = pdf.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the normal distribution\n",
    "def plot_dist():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(3,2,1)\n",
    "    plt.hist(pdf[\"age\"], bins = 40)\n",
    "    plt.title('age')\n",
    "\n",
    "    plt.subplot(3,2,2)\n",
    "    plt.hist(pdf[\"duration\"], bins = 40)\n",
    "    plt.title('duration')\n",
    "\n",
    "    plt.subplot(3,2,3)\n",
    "    plt.hist(pdf[\"campaign\"], bins = 40)\n",
    "    plt.title('campaign')\n",
    "    \n",
    "    plt.subplot(3,2,4)\n",
    "    plt.hist(pdf[\"pdays\"], bins = 40)\n",
    "    plt.title('pdays')\n",
    "         \n",
    "plot_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76a8d3",
   "metadata": {},
   "source": [
    "## ordinal data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for converting categorical textual data entries\n",
    "# conversion into categorical numeric values on basis on job profile\n",
    "job_dict = {\"entrepreneur\":11, \"self-employed\":10, \"admin\":9, \"management\":8, \"services\":7, \n",
    "       \"technician\":6, \"blue-collar\":5, \"housemaid\":4, \"retired\":3, \"student\":2, \"unemployed\":1}\n",
    "\n",
    "for key, value in job_dict.items():\n",
    "    bank_data = bank_data.withColumn(\"job\", when(bank_data[\"job\"] == key, int(value)).otherwise(bank_data[\"job\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion into categorical numeric values on basis on marital status\n",
    "marital_dict = {\"married\":3, \"single\":2, \"divorced\":1}\n",
    "\n",
    "for key, value in marital_dict.items():\n",
    "    bank_data = bank_data.withColumn(\"marital\", when(bank_data[\"marital\"] == key, value).otherwise(bank_data[\"marital\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion into categorical numeric values on basis on education\n",
    "edu_dict = {\"professional_course\":10, \"university_degree\":9, \"tertiary\":8, \"secondary\":7, \n",
    "       \"high_school\":6, \"basic_9y\":5, \"basic_6y\":4, \"primary\":3, \"basic_4y\":2, \"illiterate\":1}\n",
    "\n",
    "for key, value in edu_dict.items():\n",
    "    bank_data = bank_data.withColumn(\"education\", when(bank_data[\"education\"] == key, value).otherwise(bank_data[\"education\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {\"yes\":1, \"no\":0}\n",
    "\n",
    "for key, value in y_dict.items():\n",
    "    bank_data = bank_data.withColumn(\"y\", when(bank_data[\"y\"] == key, value).otherwise(bank_data[\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_data.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd910b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion of months into the quarters\n",
    "quarter_dict = {\"jan\":\"Q1\", \"feb\":\"Q1\", \"mar\":\"Q1\", \"apr\":\"Q2\", \"may\":\"Q2\", \"jun\":\"Q2\", \n",
    "                \"jul\":\"Q3\", \"aug\":\"Q3\", \"sep\":\"Q3\", \"oct\":\"Q4\", \"nov\":\"Q4\", \"dec\":\"Q4\"}\n",
    "\n",
    "for key, value in quarter_dict.items():\n",
    "    bank_data = bank_data.withColumn(\"month\", when(bank_data[\"month\"] == key, value).otherwise(bank_data[\"month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646678d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = bank_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc8610",
   "metadata": {},
   "source": [
    "### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53283a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding on the nominal data\n",
    "one_hot_cols = [\"default\", \"housing\", \"loan\"]\n",
    "\n",
    "for i in one_hot_cols:\n",
    "    bank = bank.withColumn(i, when(col(i) == \"yes\", 1).\n",
    "                              when(col(i) == \"no\", 0).\n",
    "                              otherwise(col(i)))\n",
    "    \n",
    "bank = bank.withColumn(\"contact\", when(col(\"contact\") == \"telephone\", 1)\n",
    "                                  .when(col(\"contact\") == \"cellular\", 0)\n",
    "                                  .otherwise(col(\"contact\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='month', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(bank)\n",
    "df_indexed = indexer_fitted.transform(bank)\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=['class_numeric'], outputCols=['class_onehot'])\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('class_onehot').alias('Quarter'))\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "num_categories = len(df_col_onehot.first()['Quarter'])\n",
    "cols_expanded = [(F.col('Quarter')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
    "bank_df = df_col_onehot.select(\"year\",\"age\",\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\n",
    "                                      \"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"emp_var_rate\",\"cons_price_idx\",\n",
    "                                      \"cons_conf_idx\",\"euribor_3m\",\"nr_employed\",\"y\",*cols_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160fd88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(bank_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da27886",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0df333",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67041655",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_data.drop(\"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b645f8",
   "metadata": {},
   "source": [
    "### Converting string datatype to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = bank_data.dtypes\n",
    "# Filter the list to only include the string datatype columns\n",
    "\n",
    "string_columns = [column[0] for column in column_types if column[1] == \"string\"]\n",
    "print(string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95313136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in string_columns:\n",
    "# Change the datatype of the columns to double\n",
    "    bank_data = bank_data.withColumn(cols, bank_data[cols].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d592827",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank_data.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634e6b1",
   "metadata": {},
   "source": [
    "### Outliers visualization and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea070837",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bank_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b422e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_columns = [\"age\",\"duration\",\"campaign\",\"pdays\",\"previous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defination to plot the outliers\n",
    "def plot_box():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(3,2,1)\n",
    "    out.boxplot(column=[\"age\"])\n",
    "\n",
    "    plt.subplot(3,2,2)\n",
    "    out.boxplot(column=[\"duration\"])\n",
    "\n",
    "    plt.subplot(3,2,3)\n",
    "    out.boxplot(column=[\"campaign\"])\n",
    "\n",
    "    plt.subplot(3,2,4)\n",
    "    out.boxplot(column=[\"pdays\"])\n",
    "\n",
    "    plt.subplot(3,2,5)\n",
    "    out.boxplot(column=[\"previous\"])\n",
    "           \n",
    "plot_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9569e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the outliers with the help of maximum binding limit\n",
    "max_out_limit = []\n",
    "for cols in outliers_columns:\n",
    "    quantiles = bank_data.approxQuantile(cols, [0.25, 0.5, 0.75], 0.01)\n",
    "    \n",
    "    q3 = quantiles[2]\n",
    "    q1 = quantiles[0]\n",
    "    iqr = q3 - q1\n",
    "    iqr = iqr*1.5\n",
    "    max_limit = q3 + iqr\n",
    "    min_limit = q1 - iqr\n",
    "    max_out_limit.append(max_limit)\n",
    "    \n",
    "    print(cols, \"max_limit: \",max_limit,\"      min_limit: \",min_limit)\n",
    "else:\n",
    "    print(\"------------------------------------------\")\n",
    "    print(max_out_limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(outliers_columns, max_out_limit):\n",
    "    bank_data = bank_data.withColumn(i, \n",
    "                    when((col(i) >= j), j).otherwise(col(i))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838141e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bank_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b46ca",
   "metadata": {},
   "source": [
    "## pyspark model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = bank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd4260",
   "metadata": {},
   "source": [
    "## Data Oversampling of \"yes\" label in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc584ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data oversampling function of the \"yes\" lebel in the dependent column\n",
    "def over_sample( data, oversampling_ratio ):\n",
    "    import math\n",
    "\n",
    "    # avoid changing the original object accidentally\n",
    "    new_data = data.toPandas()\n",
    "    \n",
    "\n",
    "    nums = new_data['y'].value_counts()\n",
    "    num_n = nums[0]\n",
    "    num_y = nums[1]\n",
    "    nums = len(new_data[\"y\"])\n",
    "\n",
    "    new_y = ( (1 - oversampling_ratio) / oversampling_ratio)*num_n\n",
    "   \n",
    "    loop_num = int( math.ceil(new_y/num_y) )\n",
    "\n",
    "    new_df = new_data[ new_data[\"y\"] == 1.0 ]\n",
    "   \n",
    "    for i in range(0, loop_num-1):\n",
    "       \n",
    "        # randomly select all rows from new_df\n",
    "        random_rows = new_df.sample(n = num_y, replace=True, random_state = 14)\n",
    "        \n",
    "        # append the selected row to bank_df\n",
    "        new_data = new_data.append(random_rows, ignore_index=True)\n",
    "       \n",
    "    new_data = spark.createDataFrame(new_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79923884",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_oversampled_df = over_sample(data = bank, oversampling_ratio = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b895f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for y column\n",
    "bank_oversampled_df.groupBy(\"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = bank_oversampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62f2f7",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min_max scaling to the selected columns\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "numerical_cols = ['year', 'age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'duration', \n",
    "                  'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor_3m', \n",
    "                  'nr_employed', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "#numerical_cols = ['year', 'age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'duration', \n",
    "                   #'cons_price_idx', 'Q4']\n",
    "\n",
    "\n",
    "# Create a vector assembler to combine the numerical columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features\")\n",
    "\n",
    "# Transform the DataFrame to create the numerical features vector\n",
    "bank = assembler.transform(bank)\n",
    "\n",
    "# Apply MinMaxScaler to the numerical features vector\n",
    "scaler = MinMaxScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical_features\")\n",
    "scaler_model = scaler.fit(bank)\n",
    "df = scaler_model.transform(bank)\n",
    "\n",
    "df = df.drop(\"numerical_features\")\n",
    "\n",
    "# Drop the original numerical columns and keep only the scaled numerical features\n",
    "bank = df.drop(*numerical_cols).withColumnRenamed(\"scaled_numerical_features\", \"sc_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75175849",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bank.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "banks = bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(banks.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the names of columns of the dataframe\n",
    "feature = []\n",
    "for columns in banks.columns:\n",
    "    feature.append(columns)\n",
    "else:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "banks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split: train data percentage: 70% and test data percentage: 30%\n",
    "train_data, test_data = banks.randomSplit([0.70, 0.30], seed = 14)\n",
    "display(train_data.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ba695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data counts\n",
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5faca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data counts\n",
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57721ee",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca389cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree Algorithm\n",
    "from pyspark.ml.classification import DecisionTreeClassifier as dtc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee774db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dtc(labelCol=\"y\",featuresCol='sc_features')\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "#Predict the values on test data\n",
    "dt_predictions = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt_predictions.select('prediction').toPandas()\n",
    "actual = dt_predictions.select('y').toPandas()\n",
    "\n",
    "#Shows confusion Report\n",
    "ConfusionMatrixDisplay.from_predictions(actual, dt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints Classification Report\n",
    "print(classification_report(actual, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c818bf1",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'sc_features',\n",
    "                        labelCol = 'y',\n",
    "                        maxIter=1000)\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr_predictions.select('prediction').toPandas()\n",
    "actual = lr_predictions.select('y').toPandas()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(actual, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0aba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(actual, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460b6d7",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ece8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "lsvc = LinearSVC(featuresCol = 'sc_features', labelCol = 'y', maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(train_data)\n",
    "\n",
    "svc_predictions = lsvcModel.transform(test_data)\n",
    "\n",
    "svc_pred = svc_predictions.select('prediction').toPandas()\n",
    "actual = svc_predictions.select('y').toPandas()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(actual, svc_pred)\n",
    "\n",
    "print(classification_report(actual, svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12bde2",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='y', featuresCol = 'sc_features')\n",
    "rf_model = rf.fit(train_data)\n",
    "predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f82cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prediction = rf_model.transform(test_data)\n",
    "rf_preds = rf_prediction.select('prediction').toPandas()\n",
    "actual = rf_prediction.select('y').toPandas()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(actual, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b516c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(actual, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c81919",
   "metadata": {},
   "source": [
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7bef9",
   "metadata": {},
   "source": [
    "### Grid search_CV for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(rf.numTrees, [20, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=lr.getLabelCol())\n",
    "\n",
    "# Define cross-validator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit cross-validator to training data\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Evaluate model on test data\n",
    "gcv_pred = cv_model.transform(test_data)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_preds = gcv_pred.select('prediction').toPandas()\n",
    "gcv_actual = gcv_pred.select('y').toPandas()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(gcv_actual, gcv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b77a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gcv_actual, gcv_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a868f17",
   "metadata": {},
   "source": [
    "### Grid search_CV for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.maxBins, [16, 32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=dt.getLabelCol())\n",
    "\n",
    "# Define cross-validator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit cross-validator to training data\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Evaluate model on test data\n",
    "gcv_dt_pred = cv_model.transform(test_data)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaeadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_preds = gcv_dt_pred.select('prediction').toPandas()\n",
    "gcv_actual = gcv_dt_pred.select('y').toPandas()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(gcv_actual, gcv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84611862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gcv_actual, gcv_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed29bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
